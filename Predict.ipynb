{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1aar4sTosV6UQo0Lm3VK_SOAhq70lW2RD","authorship_tag":"ABX9TyPrYMOs14C+7HZrg6EO9XCx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SvyqzMMyT_CV","executionInfo":{"status":"ok","timestamp":1704134243966,"user_tz":-60,"elapsed":12259,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"d26d8523-e9eb-4946-81ac-98e6186de3d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import scipy\n","\n","import re\n","\n","\n","\n","from string import punctuation\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import stopwords\n","\n","\n","\n","from sklearn.metrics import f1_score, classification_report, accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n","from sklearn import linear_model, metrics\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import log_loss, roc_auc_score, confusion_matrix\n","\n","import gensim.downloader as api\n","\n","\n","import nltk\n","nltk.download('stopwords')\n","\n","\n","import gensim\n","import itertools\n","\n","\n","from gensim.models import KeyedVectors\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.layers import Layer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","\n","\n","\n","\n","import os\n","import matplotlib\n","\n","import tensorflow as tf\n","\n","from time import time\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.optimizers import Adam  # Corrected import\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.layers import Input, Embedding, LSTM\n","\n","matplotlib.use('Agg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtO2jIo7UEgf","executionInfo":{"status":"ok","timestamp":1704134432999,"user_tz":-60,"elapsed":5711,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"4b106da7-b66d-4e33-e1db-4f5e58fe9772"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["\n","def text_to_word_list(text):\n","    text = str(text)\n","    text = text.lower()\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    text = text.split()\n","\n","    return text\n","\n","def make_w2v_embeddings(df, embedding_dim=300):\n","    vocabs = {}\n","    vocabs_cnt = 0\n","    vocabs_not_w2v = {}\n","    vocabs_not_w2v_cnt = 0\n","\n","    stops = set(stopwords.words('english'))\n","\n","\n","\n","    word2vec_model_name = 'word2vec-google-news-300'\n","    word2vec = api.load(word2vec_model_name)\n","\n","    for index, row in df.iterrows():\n","        if index != 0 and index % 1000 == 0:\n","            print(\"{:,} sentences embedded.\".format(index), flush=True)\n","\n","        for question in ['question1', 'question2']:\n","\n","            q2n = []\n","            for word in text_to_word_list(row[question]):\n","                if word in stops:\n","                    continue\n","                if word not in word2vec.key_to_index:\n","                    if word not in vocabs_not_w2v:\n","                        vocabs_not_w2v_cnt += 1\n","                        vocabs_not_w2v[word] = 1\n","                if word not in vocabs:\n","                    vocabs_cnt += 1\n","                    vocabs[word] = vocabs_cnt\n","                    q2n.append(vocabs_cnt)\n","                else:\n","                    q2n.append(vocabs[word])\n","\n","            df.at[index, question + '_n'] = q2n\n","\n","    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)\n","    embeddings[0] = 0\n","\n","    for word, index in vocabs.items():\n","        if word in word2vec.key_to_index:\n","            embeddings[index] = word2vec[word]\n","\n","    del word2vec\n","\n","    return df, embeddings\n","\n","\n","def split_and_zero_padding(df, max_seq_length):\n","    X = {'q1': df['question1_n'], 'q2': df['question2_n']}\n","\n","    for dataset, side in itertools.product([X], ['q1', 'q2']):\n","        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n","\n","    return dataset\n","\n","\n","\n"],"metadata":{"id":"NoO2UjtkUO66","executionInfo":{"status":"ok","timestamp":1704134436009,"user_tz":-60,"elapsed":7,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["max_seq_length = 25\n","\n","embedding_dim =300"],"metadata":{"id":"Qk4esDSxUiA4","executionInfo":{"status":"ok","timestamp":1704134453655,"user_tz":-60,"elapsed":9,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_pred = pd.DataFrame(np.c_[['Do you believe there is life after death?'] , ['Is it true that there is life after death?']] , columns = ['question1' , 'question2'])\n","\n","\n","\n","for q in ['question1', 'question2']:\n","    df_pred[q + '_n'] = df_pred[q]\n","\n","df_q2n, embeddings = make_w2v_embeddings(df_pred, embedding_dim=embedding_dim)\n","\n","X = df_q2n[['question1_n', 'question2_n']]\n","\n","X_pad = split_and_zero_padding(X, max_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3YUo15bUpzU","executionInfo":{"status":"ok","timestamp":1704135225175,"user_tz":-60,"elapsed":770172,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"fcb1100f-2520-4663-def9-4ee6b5b083ef"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/Colab_Notebooks/Projects/Similarity_Questions/siamese-lstm-weights.h5'\n","\n","# Chargez le modèle\n","loaded_model = tf.keras.models.load_model(model_path)\n","\n","prediction = loaded_model.predict([X_pad['q1'], X_pad['q2']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVZut9JFUwSc","executionInfo":{"status":"ok","timestamp":1704135225175,"user_tz":-60,"elapsed":16,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"3f92a93f-27f6-47fd-d3ce-780254ffd817"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step\n"]}]},{"cell_type":"code","source":["if float(prediction > 0.5) :\n","  print(f'The two questions are similar.')\n","else :\n","  print('There is No similarity')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l44M3SWgU0Ei","executionInfo":{"status":"ok","timestamp":1704135225176,"user_tz":-60,"elapsed":12,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"5cd3a996-9a45-48b8-90fa-4b541abf7b69"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["The two questions are similar.\n"]}]},{"cell_type":"code","source":["df_pred_2 = pd.DataFrame(np.c_[['How to learn python?'] , ['how to create the code in java?']] , columns = ['question1' , 'question2'])\n","\n","\n","\n","for q in ['question1', 'question2']:\n","    df_pred_2[q + '_n'] = df_pred_2[q]\n","\n","df_q2n, embeddings = make_w2v_embeddings(df_pred_2, embedding_dim=embedding_dim)\n","\n","X_2 = df_q2n[['question1_n', 'question2_n']]\n","\n","X_pad_2 = split_and_zero_padding(X_2, max_seq_length)"],"metadata":{"id":"4NYgHR-eWHAT","executionInfo":{"status":"ok","timestamp":1704135607842,"user_tz":-60,"elapsed":79948,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","prediction_2 = loaded_model.predict([X_pad_2['q1'], X_pad_2['q2']])\n","\n","if float(prediction_2 > 0.5) :\n","  print(f'The two questions are similar.')\n","else :\n","  print('There is No similarity')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AISGa5PsZImQ","executionInfo":{"status":"ok","timestamp":1704135607842,"user_tz":-60,"elapsed":10,"user":{"displayName":"houba mimoune","userId":"08976433613564980049"}},"outputId":"ce59592d-58e2-4150-ddd6-f4352bc8ec1a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 34ms/step\n","There is No similarity\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_Litqf7hZMnA"},"execution_count":null,"outputs":[]}]}